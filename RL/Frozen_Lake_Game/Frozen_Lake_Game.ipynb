{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Frozen Lake Game"
      ],
      "metadata": {
        "id": "LPgU-X7STx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be making use of Gym to provide us with an environment for a simple game called Frozen Lake. We'll then train an agent to play the game using Q-learning, and we'll get a playback of how the agent does after being trained.\n",
        "\n",
        "So, let's jump into the details for Frozen Lake!\n",
        "\n",
        "I've grabbed the description of the game directly from Gym's website. Let's read through it together."
      ],
      "metadata": {
        "id": "wnyh5qxBTvEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This grid is our environment where S is the agent's starting point, and it's safe. F represents the frozen surface and is also safe. H represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that's not good. Finally, G represents the goal, which is the space on the grid where the prized frisbee is located.\n",
        "\n",
        "The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise."
      ],
      "metadata": {
        "id": "HzH-TUe1ULl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SFFF\n",
        "\n",
        "FHFH\n",
        "\n",
        "FFFH\n",
        "\n",
        "HFFG"
      ],
      "metadata": {
        "id": "F0xhIxfQUP4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This grid is our environment where S is the agent's starting point, and it's safe. F represents the frozen surface and is also safe. H represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that's not good. Finally, G represents the goal, which is the space on the grid where the prized frisbee is located.\n",
        "\n",
        "The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise.\n",
        "\n",
        "State |\tDescription |\tReward\n",
        "------| ------------|-----------\n",
        "S\t| Agent's starting point - safe |\t0\n",
        "F\t| Frozen surface - safe\t| 0\n",
        "H\t| Hole - game over\t| 0\n",
        "G\t| Goal - game over\t| 1"
      ],
      "metadata": {
        "id": "A3cSwb0kUViR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6r-MWiES5KL",
        "outputId": "4b5f7b98-575e-4b8b-86e7-ec537f474aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "6xGg2T8pS-oY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the environment\n",
        "Next, to create our environment, we just call gym.make() and pass a string of the name of the environment we want to set up. We'll be using the environment `FrozenLake-v1`."
      ],
      "metadata": {
        "id": "xkWCZergVjie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', render_mode='ansi')"
      ],
      "metadata": {
        "id": "I0tijdfMVDS3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this env object, we're able to query for information about the environment, sample states and actions, retrieve rewards, and have our agent navigate the frozen lake. That's all made available to us conveniently with Gym."
      ],
      "metadata": {
        "id": "hXMV_kSHV8hX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Q-table\n",
        "\n",
        "We're now going to construct our Q-table, and initialize all the Q-values to zero for each state-action pair.\n",
        "\n",
        "Remember, the number of rows in the table is equivalent to the size of the state space in the environment, and the number of columns is equivalent to the size of the action space. We can get this information using using `env.observation_space.n` and `env.action_space.n`, as shown below. We can then use this information to build the Q-table and fill it with zeros."
      ],
      "metadata": {
        "id": "GEJGkl6MWitm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "\n",
        "q_table = np.zeros((state_space_size, action_space_size))"
      ],
      "metadata": {
        "id": "mEBZthXFVgH9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's our Q-table!"
      ],
      "metadata": {
        "id": "i08zlvl_W-Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJeyz9ovW2dx",
        "outputId": "af23dd90-b2cd-4f0e-9914-f66fd3c21ed5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing Q-learning parameters\n",
        "\n",
        "Now, we're going to create and initialize all the parameters needed to implement the Q-learning algorithm."
      ],
      "metadata": {
        "id": "lcfZABoTXGWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, with `num_episodes`, we define the total number of episodes we want the agent to play during training. Then, with `max_steps_per_episode`, we define a maximum number of steps that our agent is allowed to take within a single episode. So, if by the one-hundredth step, the agent hasn't reached the frisbee or fallen through a hole, then the episode will terminate with the agent receiving zero points."
      ],
      "metadata": {
        "id": "g7JC97TnY1fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 10000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "learning_rate = 0.1\n",
        "discount_rate = 0.99\n",
        "\n",
        "exploration_rate = 1\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001"
      ],
      "metadata": {
        "id": "LvHpnG2UXCJZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding the Q-learning algorithm training loop\n",
        "\n"
      ],
      "metadata": {
        "id": "PF6s7e1kYy-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we create this list to hold all of the rewards we'll get from each episode. This will be so we can see how our game score changes over time."
      ],
      "metadata": {
        "id": "Fmyht6HjaVxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_all_episodes = []"
      ],
      "metadata": {
        "id": "O9WT2tBNaVF9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # initialize new episode params\n",
        "    state = env.reset()[0]\n",
        "\n",
        "    done = False\n",
        "    rewards_current_episode = 0\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # Exploration-exploitation trade-off\n",
        "        exploration_rate_threshold = random.uniform(0,1)\n",
        "        # Take new action\n",
        "        if(exploration_rate_threshold > exploration_rate):\n",
        "          action = np.argmax(q_table[state, :])\n",
        "        else:\n",
        "          action = env.action_space.sample()\n",
        "\n",
        "        new_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        # Update Q-table for Q(s,a)\n",
        "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
        "    learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
        "\n",
        "        # Set new state\n",
        "        state = new_state\n",
        "\n",
        "        # Add new reward\n",
        "        rewards_current_episode += reward\n",
        "\n",
        "        if done == True:\n",
        "          break\n",
        "\n",
        "    # Exploration rate decay\n",
        "    exploration_rate = min_exploration_rate + \\\n",
        "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
        "\n",
        "    # Add current episode reward to total rewards list\n",
        "    rewards_all_episodes.append(rewards_current_episode)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZIMoPdW0XKAy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After all episodes complete\n",
        "\n",
        "After all episodes are finished, we now just calculate the average reward per thousand episodes from our list that contains the rewards for all episodes so that we can print it out and see how the rewards changed over time."
      ],
      "metadata": {
        "id": "MlHCHQ6BfzW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print the average reward per thousand episodes\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
        "count = 1000\n",
        "\n",
        "print(\"********Average reward per thousand episodes********\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(count, \": \", str(sum(r/1000)))\n",
        "    count += 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvtb2Pd-cPRK",
        "outputId": "9698e4dc-813f-4aa4-f9e5-31c0342dd39f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.20500000000000015\n",
            "2000 :  0.6970000000000005\n",
            "3000 :  0.6850000000000005\n",
            "4000 :  0.6820000000000005\n",
            "5000 :  0.6860000000000005\n",
            "6000 :  0.6680000000000005\n",
            "7000 :  0.6720000000000005\n",
            "8000 :  0.6840000000000005\n",
            "9000 :  0.6780000000000005\n",
            "10000 :  0.6920000000000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting the training results\n",
        "\n",
        "Let's take a second to understand how we can interpret these results. Our agent played 10,000 episodes. At each time step within an episode, the agent received a reward of 1 if it reached the frisbee, otherwise, it received a reward of 0. If the agent did indeed reach the frisbee, then the episode finished at that time-step.\n",
        "\n",
        "So, that means for each episode, the total reward received by the agent for the entire episode is either 1 or 0. So, for the first thousand episodes, we can interpret this score as meaning that **20%** of the time, the agent received a reward of 1 and won the episode. And by the last thousand episodes from a total of 10,000, the agent was winning **70%** of the time.\n",
        "\n",
        "By analyzing the grid of the game, we can see it's a lot more likely that the agent would fall in a hole or perhaps reach the max time steps than it is to reach the frisbee, so reaching the frisbee **70%** of the time isn't too shabby, especially since the agent had no explicit instructions to reach the frisbee. It learned that this is the correct thing to do."
      ],
      "metadata": {
        "id": "5FN65OnLgsc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updated Q-table"
      ],
      "metadata": {
        "id": "F-wGpz90hNcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n********Q-table********\\n\")\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_edWTbRcf9XR",
        "outputId": "4ab0b909-f825-4b89-da02-859d3a42568b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "********Q-table********\n",
            "\n",
            "[[0.54765181 0.50081325 0.4985004  0.51667671]\n",
            " [0.17923753 0.1252865  0.18409783 0.45999579]\n",
            " [0.4088647  0.16823971 0.16274984 0.19609939]\n",
            " [0.11039722 0.         0.         0.        ]\n",
            " [0.56265466 0.2877087  0.28737066 0.34555136]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.15074044 0.07191357 0.43576494 0.03462862]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.27141721 0.44754787 0.42663396 0.59721602]\n",
            " [0.50169291 0.64967969 0.4925115  0.53327344]\n",
            " [0.67070253 0.34581851 0.30710393 0.40116643]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.45914988 0.65523179 0.70612883 0.50028482]\n",
            " [0.7012334  0.83958548 0.72021309 0.7409227 ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Watch the agent play the game"
      ],
      "metadata": {
        "id": "16ypW1kOh5ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Watch our agent play Frozen Lake by playing the best action\n",
        "# from each state according to the Q-table\n",
        "\n",
        "for episode in range(3):\n",
        "    # initialize new episode params\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "\n",
        "    clear_output(wait = True)\n",
        "    print(\"**** EPISODE \", episode + 1, \" ****\\n\\n\\n\")\n",
        "    time.sleep(1)\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "      clear_output(wait = True)\n",
        "      # Show current state of environment on screen\n",
        "      print(env.render())\n",
        "      time.sleep(0.3)\n",
        "\n",
        "      # Choose action with highest Q-value for current state\n",
        "      action = np.argmax(q_table[state, :])\n",
        "      # Take new action\n",
        "      new_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "      if done:\n",
        "        clear_output(wait = True)\n",
        "        print(env.render())\n",
        "        if reward == 1:\n",
        "            # Agent reached the goal and won episode\n",
        "            print(\"*** You reached the goal! ***\")\n",
        "            time.sleep(2)\n",
        "        else:\n",
        "            # Agent stepped in a hole and lost episode\n",
        "            print(\"*** You fell through a hole! ***\")\n",
        "            time.sleep(2)\n",
        "        break\n",
        "\n",
        "      # Set new state\n",
        "      state = new_state\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBR2rS04hRwd",
        "outputId": "61aeecd2-fc31-4cf7-ae0f-720c13037384"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "*** You reached the goal! ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USq7HT_Akd1F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}